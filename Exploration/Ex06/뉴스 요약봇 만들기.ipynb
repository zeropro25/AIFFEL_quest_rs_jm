{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b4ed961-758d-4a76-9775-444345adbf66",
   "metadata": {},
   "source": [
    "## ë‰´ìŠ¤ ìš”ì•½ë´‡ ë§Œë“¤ê¸° ğŸ¤–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f171ac-76aa-4ffb-9600-3c58460b53a0",
   "metadata": {},
   "source": [
    "### 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° íŒ¨í‚¤ì§€ ì¤€ë¹„\n",
    "- NLTK : ì˜ì–´ ê¸°í˜¸, í†µê³„, ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
    "- NLTKì—ëŠ” I, my, me, over, ì¡°ì‚¬, ì ‘ë¯¸ì‚¬ì™€ ê°™ì´ ë¬¸ì¥ì—ëŠ” ìì£¼ ë“±ì¥í•˜ì§€ë§Œ, ì˜ë¯¸ë¥¼ ë¶„ì„í•˜ê³  ìš”ì•½í•˜ëŠ” ë°ëŠ” ê±°ì˜ ì˜ë¯¸ê°€ ì—†ëŠ” 100ì—¬ê°œì˜ ë¶ˆìš©ì–´ê°€ ë¯¸ë¦¬ ì •ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dbe8a7-3ad9-4f53-8465-fb88f39c548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.12/site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b7de5-3a43-4253-805a-300291324aa1",
   "metadata": {},
   "source": [
    "- BeautifulSoup : ë¬¸ì„œë¥¼ íŒŒì‹± í•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” íŒ¨í‚¤ì§€ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c27ecf9-7839-45f7-95de-91213be8daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5396f52c-669e-4c6e-b7e7-dea0e4d13c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTKì—ì„œ ë¶ˆìš©ì–´ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ì— í•„ìš”í•œ íŒ¨í‚¤ì§€ ë‹¤ìš´ë¡œë“œ\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "import urllib.request\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c6d88b-8c1a-4095-bcdf-ea2e329e13f9",
   "metadata": {},
   "source": [
    "### 2. ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°\n",
    "- ì´ë¯¸ ë°ì´í„°ë¥¼ í•œ ë²ˆ ë¶ˆëŸ¬ì™”ì—ˆê¸° ë•Œë¬¸ì— ì£¼ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.\n",
    "- ì¶”ìƒì  ìš”ì•½ì„ í•  ë•ŒëŠ” textë¥¼ ë³¸ë¬¸, headlinesë¥¼ ì´ë¯¸ ìš”ì•½ëœ ë°ì´í„°ë¡œ ê°„ì£¼í•˜ê³  í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- ì¶”ì¶œì  ìš”ì•½ì„ í•  ë•Œì—ëŠ” text ì—´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044915fa-5968-44d3-9379-18cf0f3ce8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import urllib.request\n",
    "#urllib.request.urlretrieve(\"https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\", filename=\"news_summary_more.csv\")\n",
    "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177feaf4-bc59-4828-85f1-713b20450e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ì „ì²´ ìƒ˜í”Œìˆ˜ :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35590dc-eb5e-4b1c-ae57-5786b7d4e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55392ac-226e-4a84-8633-226207a661c4",
   "metadata": {},
   "source": [
    "- ë°ì´í„° ì—´ ì¤‘ì—ì„œ Summaryì™€ Text ì—´ë§Œ ë³„ë„ë¡œ ì €ì¥í•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- ì°¸ê³ ë¡œ Text ì—´ì˜ ë‚´ìš©ì„ ìš”ì•½í•œ ê²ƒì´ Summary ì—´ì…ë‹ˆë‹¤.\n",
    "- Text ì‹œí€€ìŠ¤ë¥¼ ì…ë ¥ ë°›ìœ¼ë©´ Summary ì‹œí€€ìŠ¤ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í›ˆë ¨í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218749d5-4204-41fe-be16-df0fe3b6a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['text', 'headlines']]\n",
    "data.head()\n",
    "\n",
    "#ëœë¤í•œ 15ê°œ ìƒ˜í”Œ ì¶œë ¥\n",
    "data.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefcc4cd-9e69-4686-9ec1-13647127e179",
   "metadata": {},
   "source": [
    "### 3. ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸° (ì¶”ìƒì  ìš”ì•½)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bd67fc-035b-40f5-b98b-8873a08788ad",
   "metadata": {},
   "source": [
    "- ì¤‘ë³µ ìƒ˜í”Œê³¼ NULL ê°’ì´ ì¡´ì¬í•˜ëŠ” ìƒ˜í”Œì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "- ```drop_duplicates()```ë¥¼ ì‚¬ìš©í•´ì„œ ì¤‘ë³µ ìƒ˜í”Œì„ ì œê±°í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad0dbca-0866-4374-a9dd-946c02450f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('text ì—´ì—ì„œ ì¤‘ë³µì„ ë°°ì œí•œ ìœ ì¼í•œ ìƒ˜í”Œì˜ ìˆ˜ :', data['text'].nunique())\n",
    "print('headlines ì—´ì—ì„œ ì¤‘ë³µì„ ë°°ì œí•œ ìœ ì¼í•œ ìƒ˜í”Œì˜ ìˆ˜ :', data['headlines'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4103f226-f75f-4ef3-9e0a-82e2a23622f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inplace=True ë¥¼ ì„¤ì •í•˜ë©´ DataFrame íƒ€ì… ê°’ì„ return í•˜ì§€ ì•Šê³  data ë‚´ë¶€ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤.\n",
    "\n",
    "data.drop_duplicates(subset = ['text'], inplace=True)\n",
    "print('ì „ì²´ ìƒ˜í”Œìˆ˜ :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd412b37-deae-4558-95dd-e2c1101a681a",
   "metadata": {},
   "source": [
    "- Null ê°’ í•œ ê°œê°€ ë‚¨ì•„ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ```.isnull().sum()```ì„ í†µí•´ Null ê°’ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì œê±°í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278b3973-f1ad-4c7d-9a9c-3ad5fe49c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c1ffc7-3c9f-4916-90df-55ca17f878a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('ì „ì²´ ìƒ˜í”Œìˆ˜ :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531ddc62-8bae-428a-bd86-a782b9ac914e",
   "metadata": {},
   "source": [
    "- í…ìŠ¤íŠ¸ ì •ê·œí™” : it'll = it will, must'n = must not ë“± ì´ëŸ¬í•œ í‘œí˜„ì„ ê°™ì€ í‘œí˜„ìœ¼ë¡œ í†µì¼ì„ ì‹œì¼œ ê¸°ê³„ì˜ ì—°ì‚°ëŸ‰ì„ ì¤„ì´ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n",
    "- í…ìŠ¤íŠ¸ ì •ê·œí™”ë¥¼ ìœ„í•´ ì‚¬ì „ì„ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf216d-155b-431e-b065-e1502621fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"ì •ê·œí™” ì‚¬ì „ì˜ ìˆ˜: \", len(contractions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e578e363-b079-446d-8cb7-c8d6df63d5b7",
   "metadata": {},
   "source": [
    "- NLTKì—ì„œ ì œê³µí•˜ëŠ” ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¸ì¡°í•´ì„œ, ìƒ˜í”Œì˜ ë¶ˆìš©ì–´ë¥¼ ì œê±°í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a57c8c-0cf0-4c7a-839f-1c8055559bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ë¶ˆìš©ì–´ ê°œìˆ˜ :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4302f33e-c429-4f53-aad6-fa226ed4d209",
   "metadata": {},
   "source": [
    "- ë¶ˆìš©ì–´ ì œê±° + ëª¨ë“  ì˜ì–´ ë¬¸ì ì†Œë¬¸ìí™” + HTML íƒœê·¸ ì œê±° + íŠ¹ìˆ˜ ë¬¸ì ì œê±°ë¥¼ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.\n",
    "- ë‹¤ë§Œ text ì „ì²˜ë¦¬ ì‹œì—ë§Œ í˜¸ì¶œí•˜ê³  headlinesëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ ìš”ì•½ ê²°ê³¼ë¥¼ ìœ„í•´ ë¶ˆìš©ì–´ë¥¼ ì‚­ì œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8258d5e-8db1-4bad-bc95-52033c0ee101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # í…ìŠ¤íŠ¸ ì†Œë¬¸ìí™”\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> ë“±ì˜ html íƒœê·¸ ì œê±°\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # ê´„í˜¸ë¡œ ë‹«íŒ ë¬¸ìì—´ (...) ì œê±° Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # ìŒë”°ì˜´í‘œ \" ì œê±°\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # ì•½ì–´ ì •ê·œí™”\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # ì†Œìœ ê²© ì œê±°. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # ì˜ì–´ ì™¸ ë¬¸ì(ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì ë“±) ê³µë°±ìœ¼ë¡œ ë³€í™˜\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # mì´ 3ê°œ ì´ìƒì´ë©´ 2ê°œë¡œ ë³€ê²½. Ex) ummmmmmm yeah -> umm yeah\n",
    "\n",
    "    # ë¶ˆìš©ì–´ ì œê±° (text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # ë¶ˆìš©ì–´ ë¯¸ì œê±° (Headlines)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b3735d-b82e-4399-ba93-28372aeb80d1",
   "metadata": {},
   "source": [
    "- lxml ì„¤ì¹˜ í›„ì— ê¼­ ì»¤ë„ë§Œ Restart í•´ì•¼ ì˜¤ë¥˜ê°€ ë‚˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e37f5bd-4d28-428e-859b-74103ebbd303",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3ec347-7878-43a6-92a7-1d4c4be09eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
    "temp_headlines = 'Great way to start (or finish) the day!!!'\n",
    "\n",
    "print(\"text: \", preprocess_sentence(temp_text))\n",
    "print(\"headlines:\", preprocess_sentence(temp_summary, False))  # ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ee12a-7337-4670-b236-185704136ade",
   "metadata": {},
   "source": [
    "- ì „ì²´ text ë°ì´í„°ì— ëŒ€í•œ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8f4dcd-44cd-4aac-aa92-d0c614c57bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ Text ë°ì´í„°ì— ëŒ€í•œ ì „ì²˜ë¦¬ : 10ë¶„ ì´ìƒ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "clean_text = []\n",
    "\n",
    "for sentence in data['text']:\n",
    "    clean_text.append(preprocess_sentence(sentence))\n",
    "\n",
    "# ì „ì²˜ë¦¬ í›„ ì¶œë ¥\n",
    "print(\"Text ì „ì²˜ë¦¬ í›„ ê²°ê³¼: \", clean_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe1b094-3d65-4c23-ae95-503e0c719547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ headlines ë°ì´í„°ì— ëŒ€í•œ ì „ì²˜ë¦¬ : 5ë¶„ ì´ìƒ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "clean_headlines = []\n",
    "\n",
    "for sentence in data['headlines']:\n",
    "    clean_summary.append(preprocess_sentence(sentence, remove_stopwords=False))\n",
    "\n",
    "print(\"headlines ì „ì²˜ë¦¬ í›„ ê²°ê³¼: \", clean_summary[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd435d2f-c40b-4974-9533-3194ee82eb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = clean_text\n",
    "data['headlines'] = clean_summary\n",
    "\n",
    "# ë¹ˆ ê°’ì„ Null ê°’ìœ¼ë¡œ ë³€í™˜\n",
    "data.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ab40f-afc8-402c-b91b-fa041222faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f5bdb-71a0-4aeb-bc13-b77ad9c8c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('ì „ì²´ ìƒ˜í”Œìˆ˜ :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6050ec-d260-43f5-9ce6-b9edb3f8dcb6",
   "metadata": {},
   "source": [
    "- textì™€ headlinesì˜ ìµœì†Œ, ìµœëŒ€, í‰ê·  ê¸¸ì´ë¥¼ êµ¬í•˜ê³  ê¸¸ì´ ë¶„í¬ë¥¼ ì‹œê°í™”í•´ì„œ ë³¸ ë‹¤ìŒ, ìƒ˜í”Œì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì •í•˜ê¸°ë¡œ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82be66-4926-4141-bc54-8ed227017526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸¸ì´ ë¶„í¬ ì¶œë ¥\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in data['text']]\n",
    "summary_len = [len(s.split()) for s in data['headlines']]\n",
    "\n",
    "print('í…ìŠ¤íŠ¸ì˜ ìµœì†Œ ê¸¸ì´ : {}'.format(np.min(text_len)))\n",
    "print('í…ìŠ¤íŠ¸ì˜ ìµœëŒ€ ê¸¸ì´ : {}'.format(np.max(text_len)))\n",
    "print('í…ìŠ¤íŠ¸ì˜ í‰ê·  ê¸¸ì´ : {}'.format(np.mean(text_len)))\n",
    "print('ìš”ì•½ì˜ ìµœì†Œ ê¸¸ì´ : {}'.format(np.min(summary_len)))\n",
    "print('ìš”ì•½ì˜ ìµœëŒ€ ê¸¸ì´ : {}'.format(np.max(summary_len)))\n",
    "print('ìš”ì•½ì˜ í‰ê·  ê¸¸ì´ : {}'.format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title('Headlines')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Summary')\n",
    "plt.hist(summary_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d9354e-573b-450f-90d0-3a30a4663ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_max_len = 50\n",
    "headlines_max_len = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5542b5-b750-41d5-b24c-f72d93e84dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s.split()) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('ì „ì²´ ìƒ˜í”Œ ì¤‘ ê¸¸ì´ê°€ %s ì´í•˜ì¸ ìƒ˜í”Œì˜ ë¹„ìœ¨: %s'%(max_len, (cnt / len(nested_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ca237-3087-46ff-a526-c7140b774e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "below_threshold_len(text_max_len, data['text'])\n",
    "below_threshold_len(summary_max_len, data['headlines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414ec01f-29e8-447f-93aa-3db445801d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "data = data[data['headlines'].apply(lambda x: len(x.split()) <= summary_max_len)]\n",
    "\n",
    "print('ì „ì²´ ìƒ˜í”Œìˆ˜ :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01134c9e-fc46-409f-8027-0280f66a7f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìš”ì•½ ë°ì´í„°ì—ëŠ” ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì¶”ê°€í•œë‹¤.\n",
    "data['decoder_input'] = data['headlines'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['headlines'].apply(lambda x : x + ' eostoken')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3865e93f-e5e3-4cb2-8066-d005c431ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = np.array(data['text']) # ì¸ì½”ë”ì˜ ì…ë ¥\n",
    "decoder_input = np.array(data['decoder_input']) # ë””ì½”ë”ì˜ ì…ë ¥\n",
    "decoder_target = np.array(data['decoder_target']) # ë””ì½”ë”ì˜ ë ˆì´ë¸”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2336d-634b-4ce5-b110-a8ef92ddb17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a993d0ad-e4fc-49ee-83b4-c529915d9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b9273f-9521-4c8a-a593-aa555ddb16fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ìˆ˜ :', n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca248f6-225e-41b0-83d3-5edfd051b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('í›ˆë ¨ ë°ì´í„°ì˜ ê°œìˆ˜ :', len(encoder_input_train))\n",
    "print('í›ˆë ¨ ë ˆì´ë¸”ì˜ ê°œìˆ˜ :', len(decoder_input_train))\n",
    "print('í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ê°œìˆ˜ :', len(encoder_input_test))\n",
    "print('í…ŒìŠ¤íŠ¸ ë ˆì´ë¸”ì˜ ê°œìˆ˜ :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916001f7-5af6-4abf-b6ba-de7419b1f51b",
   "metadata": {},
   "source": [
    "- ë‹¨ì–´ ì§‘í•©(Vocabulary) : ê¸°ê³„ê°€ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë‹¨ì–´ë“¤ì„ ëª¨ë‘ ì •ìˆ˜ë¡œ ë°”ê¿”ì•¼ í•˜ëŠ”ë°, ì´ ë•Œ ê° ë‹¨ì–´ì— ê³ ìœ í•œ ì •ìˆ˜ë¥¼ ë§µí•‘í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cbcb9-6f9a-4402-b90b-f80b6c66d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def src_tokenizer(text): # í† í¬ë‚˜ì´ì € ì •ì˜\n",
    "    text = text.lower()  # ì†Œë¬¸ìë¡œ ë³€í™˜\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    return text.split()  # ê³µë°± ê¸°ì¤€ í† í°í™”\n",
    "\n",
    "def build_vocab(texts):\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # íŒ¨ë”©ê³¼ UNK í† í° ì¶”ê°€\n",
    "    word_counter = Counter()\n",
    "\n",
    "    for text in texts:\n",
    "        word_counter.update(src_tokenizer(text))  # ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "\n",
    "    # ë‹¨ì–´ ì§‘í•© ìƒì„± (ë¹ˆë„ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ)\n",
    "    for word, _ in word_counter.most_common():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "src_vocab = build_vocab(encoder_input_train) # ì…ë ¥ëœ ë°ì´í„°ë¡œë¶€í„° ë‹¨ì–´ ì§‘í•© ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f3b808-c4af-4253-a35c-8c65f0944b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë“±ì¥ ë¹ˆë„ ìˆ˜ê°€ 7íšŒ ë¯¸ë§Œì¸ ë‹¨ì–´ë“¤ì´ ì´ ë°ì´í„°ì—ì„œ ì–¼ë§Œí¼ì˜ ë¹„ì¤‘ì„ ì°¨ì§€í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "threshold = 7\n",
    "\n",
    "# ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©\n",
    "text_data = data['text'].tolist()\n",
    "summary_data = data['headlines'].tolist()\n",
    "# ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "word_counter = Counter()\n",
    "for text in text_data:\n",
    "    word_counter.update(text.split())\n",
    "\n",
    "total_cnt = len(word_counter)  # ì „ì²´ ë‹¨ì–´ ê°œìˆ˜\n",
    "total_freq = sum(word_counter.values())  # ì „ì²´ ë‹¨ì–´ ë“±ì¥ íšŸìˆ˜\n",
    "rare_cnt = sum(1 for count in word_counter.values() if count < threshold)  # í¬ê·€ ë‹¨ì–´ ê°œìˆ˜\n",
    "rare_freq = sum(count for count in word_counter.values() if count < threshold)  # í¬ê·€ ë‹¨ì–´ ë“±ì¥ íšŸìˆ˜\n",
    "\n",
    "# í¬ê·€ ë‹¨ì–´ë¥¼ ì œì™¸í•œ ë‹¨ì–´ ì‚¬ì „ êµ¬ì¶•\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # íŒ¨ë”© ë° ë¯¸ë“±ë¡ ë‹¨ì–´ ì¶”ê°€\n",
    "word_index = {word: idx + 2 for idx, (word, count) in enumerate(word_counter.items()) if count >= threshold}\n",
    "\n",
    "print('ë‹¨ì–´ ì§‘í•©(vocabulary)ì˜ í¬ê¸° :', total_cnt)\n",
    "print('ë“±ì¥ ë¹ˆë„ê°€ %së²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: %s'%(threshold - 1, rare_cnt))\n",
    "print('ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ë¥¼ ì œì™¸ì‹œí‚¬ ê²½ìš°ì˜ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° %s'%(total_cnt - rare_cnt))\n",
    "print(\"ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c58c6c8-309d-49ae-976d-cd5c807ba19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 8000\n",
    "\n",
    "def build_limited_vocab(texts, vocab_size):\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # íŒ¨ë”©ê³¼ UNK í† í° ì¶”ê°€\n",
    "    word_counter = Counter()\n",
    "\n",
    "    for text in texts:\n",
    "        word_counter.update(src_tokenizer(text))  # ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "\n",
    "    # ë¹ˆë„ê°€ ë†’ì€ ìƒìœ„ vocab_size - 2ê°œ ë‹¨ì–´ë§Œ ì„ íƒ (PAD, UNK í¬í•¨)\n",
    "    for word, _ in word_counter.most_common(vocab_size - 2):\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "src_vocab = build_limited_vocab(encoder_input_train, src_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb247c6-c155-4225-83ed-f9211d102863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(texts, vocab):\n",
    "    sequences = []\n",
    "    for text in texts:\n",
    "        sequence = [vocab.get(word, vocab[\"<UNK>\"]) for word in src_tokenizer(text)]\n",
    "        sequences.append(sequence)\n",
    "    return sequences\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë°ì´í„° ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "encoder_input_train_seq = text_to_sequence(encoder_input_train, src_vocab)\n",
    "encoder_input_test_seq = text_to_sequence(encoder_input_test, src_vocab)\n",
    "\n",
    "# ì˜ ì§„í–‰ë˜ì—ˆëŠ”ì§€ ìƒ˜í”Œ ì¶œë ¥\n",
    "print(encoder_input_train_seq[:3])\n",
    "print(encoder_input_test_seq[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9e0913-3811-4744-a694-0a8030d1852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tar_tokenizer(text):\n",
    "    text = text.lower()  # ì†Œë¬¸ìë¡œ ë³€í™˜\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    return text.split()  # ê³µë°± ê¸°ì¤€ í† í°í™”\n",
    "\n",
    "tar_vocab = build_vocab(decoder_input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b40774c-75aa-42aa-bf06-13412df9cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 6\n",
    "\n",
    "word_counter = Counter()\n",
    "for text in decoder_input_train:\n",
    "    word_counter.update(tar_tokenizer(text))  # ê° ë¬¸ì¥ì˜ ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°\n",
    "\n",
    "# ì „ì²´ ë‹¨ì–´ ê°œìˆ˜ ë° ë“±ì¥ ë¹ˆë„ ê³„ì‚°\n",
    "total_cnt = len(word_counter)  # ì „ì²´ ë‹¨ì–´ ê°œìˆ˜\n",
    "total_freq = sum(word_counter.values())  # ì „ì²´ ë‹¨ì–´ ë“±ì¥ íšŸìˆ˜\n",
    "rare_cnt = sum(1 for count in word_counter.values() if count < threshold)  # í¬ê·€ ë‹¨ì–´ ê°œìˆ˜\n",
    "rare_freq = sum(count for count in word_counter.values() if count < threshold)  # í¬ê·€ ë‹¨ì–´ ë“±ì¥ íšŸìˆ˜\n",
    "\n",
    "print('ë‹¨ì–´ ì§‘í•©(vocabulary)ì˜ í¬ê¸° :', total_cnt)\n",
    "print('ë“±ì¥ ë¹ˆë„ê°€ %së²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: %s'%(threshold - 1, rare_cnt))\n",
    "print('ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ë¥¼ ì œì™¸ì‹œí‚¬ ê²½ìš°ì˜ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° %s'%(total_cnt - rare_cnt))\n",
    "print(\"ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b1b0a-dd6b-443f-baae-737fc00ec351",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_vocab_size = 2000\n",
    "tar_vocab = build_limited_vocab(decoder_input_train + decoder_target_train, tar_vocab_size)\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "decoder_input_train_seq = text_to_sequence(decoder_input_train, tar_vocab)\n",
    "decoder_target_train_seq = text_to_sequence(decoder_target_train, tar_vocab)\n",
    "decoder_input_test_seq = text_to_sequence(decoder_input_test, tar_vocab)\n",
    "decoder_target_test_seq = text_to_sequence(decoder_target_test, tar_vocab)\n",
    "\n",
    "# ì˜ ë³€í™˜ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "print('input')\n",
    "print('input ',decoder_input_train_seq[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train_seq[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d3ef1b-a76e-4c4b-b063-047285cacacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_vocab_size = 2000\n",
    "tar_vocab = build_limited_vocab(decoder_input_train + decoder_target_train, tar_vocab_size)\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "decoder_input_train_seq = text_to_sequence(decoder_input_train, tar_vocab)\n",
    "decoder_target_train_seq = text_to_sequence(decoder_target_train, tar_vocab)\n",
    "decoder_input_test_seq = text_to_sequence(decoder_input_test, tar_vocab)\n",
    "decoder_target_test_seq = text_to_sequence(decoder_target_test, tar_vocab)\n",
    "\n",
    "# ì˜ ë³€í™˜ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "print('input')\n",
    "print('input ',decoder_input_train_seq[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train_seq[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ed9fc7-20cc-489a-a293-1ebdc29383f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('ì‚­ì œí•  í›ˆë ¨ ë°ì´í„°ì˜ ê°œìˆ˜ :', len(drop_train))\n",
    "print('ì‚­ì œí•  í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ê°œìˆ˜ :', len(drop_test))\n",
    "\n",
    "encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]\n",
    "decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]\n",
    "decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]\n",
    "\n",
    "encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]\n",
    "decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]\n",
    "decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]\n",
    "\n",
    "print('í›ˆë ¨ ë°ì´í„°ì˜ ê°œìˆ˜ :', len(encoder_input_train))\n",
    "print('í›ˆë ¨ ë ˆì´ë¸”ì˜ ê°œìˆ˜ :', len(decoder_input_train))\n",
    "print('í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ê°œìˆ˜ :', len(encoder_input_test))\n",
    "print('í…ŒìŠ¤íŠ¸ ë ˆì´ë¸”ì˜ ê°œìˆ˜ :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa7dfe1-92c9-4dbd-a570-f8ac745d6a47",
   "metadata": {},
   "source": [
    "- í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í–ˆìœ¼ë¯€ë¡œ, ì´ì œ ì„œë¡œ ë‹¤ë¥¸ ê¸¸ì´ì˜ ìƒ˜í”Œë“¤ì„ ë³‘ë ¬ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ê°™ì€ ê¸¸ì´ë¡œ ë§ì¶°ì£¼ê¸° ìœ„í•´ íŒ¨ë”© ì‘ì—…ì„ í•´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e2b80-3563-409b-b0d6-ec0bfb188b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# í…ì„œ ë³€í™˜ í•¨ìˆ˜ (ë¦¬ìŠ¤íŠ¸ â†’ PyTorch í…ì„œ)\n",
    "def convert_to_tensor(sequences):\n",
    "    return [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "\n",
    "# íŒ¨ë”© ì ìš© í•¨ìˆ˜ (PyTorch `pad_sequence()` í™œìš©)\n",
    "def pad_sequences_pytorch(sequences, maxlen, padding_value=0):\n",
    "    sequences = convert_to_tensor(sequences)  # ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=True, padding_value=padding_value)  # íŒ¨ë”© ì ìš©\n",
    "    return padded_seqs[:, :maxlen]  # maxlen ê¸¸ì´ë¡œ ìë¥´ê¸° (ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ ë°©ì§€)\n",
    "\n",
    "# íŒ¨ë”© ì ìš©\n",
    "encoder_input_train = pad_sequences_pytorch(encoder_input_train_seq, maxlen=text_max_len)\n",
    "encoder_input_test = pad_sequences_pytorch(encoder_input_test_seq, maxlen=text_max_len)\n",
    "decoder_input_train = pad_sequences_pytorch(decoder_input_train_seq, maxlen=summary_max_len)\n",
    "decoder_target_train = pad_sequences_pytorch(decoder_target_train_seq, maxlen=summary_max_len)\n",
    "decoder_input_test = pad_sequences_pytorch(decoder_input_test_seq, maxlen=summary_max_len)\n",
    "decoder_target_test = pad_sequences_pytorch(decoder_target_test_seq, maxlen=summary_max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5052dd2-a522-4747-bcb1-e0b569190426",
   "metadata": {},
   "source": [
    "### 6. ëª¨ë¸ ì„¤ê³„í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1708dc-d891-4788-8308-ff1545830986",
   "metadata": {},
   "source": [
    "- ì¸ì½”ë” : ì…ë ¥ ë¬¸ì¥ì„ ì„ë² ë”©í•˜ê³  LSTMì„ ê±°ì³ ë¬¸ë§¥ ì •ë³´(hidden state, cell state)ë¡œ ì••ì¶•í•©ë‹ˆë‹¤. (Output : ê° ì‹œì ë³„ LSTM ì¶œë ¥ / Hidden : ë§ˆì§€ë§‰ ì‹œì ì˜ Hidden state (ë””ì½”ë” ì´ˆê¸° ìƒíƒœë¡œ ì‚¬ìš©) / Cell : ë§ˆì§€ë§‰ ì‹œì ì˜ Cell state (ë””ì½”ë” ì´ˆê¸° ìƒíƒœë¡œ ì‚¬ìš©)\n",
    "\n",
    "- ë””ì½”ë” : ì´ì „ ì‹œì ì˜ ë‹¨ì–´ ë˜ëŠ” SOS í† í°ì„ ì…ë ¥ ë°›ì•„ LSTMìœ¼ë¡œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤. ì¸ì½”ë”ì˜ Hidden, Cellì„ ì´ˆê¸° ìƒíƒœë¡œ ë°›ì•„ì„œ ë¬¸ì¥ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
    "\n",
    "- ì‹œí€€ìŠ¤ íˆ¬ ì‹œí€€ìŠ¤ : ì¸ì½”ë”ë¡œ ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì¸ì½”ë”©í•˜ê³ , ë””ì½”ë”ë¡œ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ í•œ ì‹œì ì‹ ë””ì½”ë”©í•©ë‹ˆë‹¤. ë””ì½”ë”ì˜ ê° ì‹œì  ì¶œë ¥ì„ Linear Layerì— í†µê³¼ì‹œì¼œ ë‹¨ì–´ ë¶„ë¥˜ í™•ë¥  ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "[ì…ë ¥ ë¬¸ì¥] â†’ Encoder â†’ (hidden, cell) â”€â”\n",
    "                                        â†“\n",
    "                              Decoder (ë°˜ë³µ) â†’ Linear Layer â†’ ë‹¨ì–´ í™•ë¥ \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1342810e-75f2-4a78-a7ca-7328f67300b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ì¸ì½”ë” ì„¤ê³„ ì‹œì‘\n",
    "embedding_dim = 128\n",
    "hidden_size = 256 # LSTMì—ì„œ ì–¼ë§Œí¼ì˜ ìˆ˜ìš©ë ¥(Capacity)ì„ ê°€ì§ˆì§€ ì •í•˜ëŠ” íŒŒë¼ë¯¸í„°\n",
    "src_vocab_size = len(src_vocab)  # ë‹¨ì–´ ì§‘í•© í¬ê¸°\n",
    "\n",
    "# ì¸ì½”ë”\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=3, dropout=0.4):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_size, num_layers=num_layers,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x): # ì¸ì½”ë”ì˜ ì„ë² ë”© ì¸µ\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded)  # LSTM ì‹¤í–‰\n",
    "        return output, hidden, cell\n",
    "\n",
    "# ì¸ì½”ë” ëª¨ë¸ ìƒì„±\n",
    "encoder = Encoder(src_vocab_size, embedding_dim, hidden_size, num_layers=3, dropout=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f12ef-eb6f-4379-a8d6-970ce3b82b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë””ì½”ë” ì„¤ê³„\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, dropout=0.4, num_layers=3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_size, num_layers=num_layers, dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hidden, cell): # ë””ì½”ë”ì˜ ì„ë² ë”© ì¸µ\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))  # ì´ˆê¸° ìƒíƒœë¥¼ ì¸ì½”ë”ì—ì„œ ì „ë‹¬ë°›ìŒ\n",
    "        return output, hidden, cell\n",
    "\n",
    "# ë””ì½”ë” ëª¨ë¸ ìƒì„±\n",
    "decoder = Decoder(tar_vocab_size, embedding_dim, hidden_size, num_layers=3, dropout=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a9d46-ddf5-4502-8c1d-1e813eccb05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë””ì½”ë”ì˜ ì¶œë ¥ì¸µ\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, vocab_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.softmax_layer = nn.Linear(hidden_size, vocab_size)  # ì¶œë ¥ì¸µ ì •ì˜\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        # ì¸ì½”ë” ì‹¤í–‰\n",
    "        encoder_output, hidden, cell = self.encoder(encoder_input)\n",
    "\n",
    "        # ë””ì½”ë” ì‹¤í–‰\n",
    "        decoder_output, _, _ = self.decoder(decoder_input, hidden, cell)\n",
    "\n",
    "        # ì¶œë ¥ì¸µ ì ìš© (SoftmaxëŠ” Loss ë‚´ë¶€ì—ì„œ ì ìš©ë˜ë¯€ë¡œ ìƒëµ ê°€ëŠ¥)\n",
    "        output = self.softmax_layer(decoder_output)\n",
    "        return output\n",
    "\n",
    "# ëª¨ë¸ ì •ì˜\n",
    "model = Seq2Seq(encoder, decoder, tar_vocab_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b8cda-f449-4a66-bd32-4ef0d7c8958b",
   "metadata": {},
   "source": [
    "- ì•„ë˜ëŠ” ê¸°ì¡´ êµ¬ì¡°ì— ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì ìš©í•©ë‹ˆë‹¤.\n",
    "- ì•„ë˜ ì½”ë“œëŠ” ì¸ì½”ë”ì˜ hidden stateë“¤ê³¼ ë””ì½”ë”ì˜ hidden stateë“¤ì„ ì–´í…ì…¥ í•¨ìˆ˜ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , ì–´í…ì…˜ í•¨ìˆ˜ê°€ ë¦¬í„´í•œ ê°’ì„ ë””ì½”ë”ì˜ hidden stateì™€ í•¨ê»˜ í™œìš©í•˜ëŠ” í˜•íƒœë¡œ ì‘ë™í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f8df50-ab47-4b2b-8c09-9b2ec1173c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_dot(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention_dot, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size, hidden_size)  # ì–´í…ì…˜ ê°€ì¤‘ì¹˜\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)  # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ë²¡í„°\n",
    "\n",
    "    def forward(self, decoder_output, encoder_outputs):\n",
    "        attn_weights = torch.bmm(decoder_output, encoder_outputs.transpose(1, 2))\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)  # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì •ê·œí™”\n",
    "        attn_out = torch.bmm(attn_weights, encoder_outputs)\n",
    "\n",
    "        return attn_out\n",
    "\n",
    "class Seq2SeqWithAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder, vocab_size, hidden_size):\n",
    "        super(Seq2SeqWithAttention, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.attention = Attention_dot(hidden_size)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)  # ì–´í…ì…˜ ê²°í•©\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size)  # ìµœì¢… ì¶œë ¥ì¸µ\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        encoder_outputs, hidden, cell = self.encoder(encoder_input)\n",
    "        decoder_outputs, _, _ = self.decoder(decoder_input, hidden, cell)\n",
    "\n",
    "        # ì–´í…ì…˜ ì ìš©\n",
    "        attn_out = self.attention(decoder_outputs, encoder_outputs)\n",
    "\n",
    "        # ì–´í…ì…˜ ê²°ê³¼ì™€ ë””ì½”ë” ì¶œë ¥ ì—°ê²°\n",
    "        decoder_concat_output = torch.cat((decoder_outputs, attn_out), dim=-1)\n",
    "\n",
    "        # ì–´í…ì…˜ ê²°í•© í›„ ìµœì¢… ì¶œë ¥\n",
    "        decoder_concat_output = torch.tanh(self.concat(decoder_concat_output))\n",
    "        output = self.output_layer(decoder_concat_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "model = Seq2SeqWithAttention(encoder, decoder, tar_vocab_size, hidden_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474fadb-d736-4732-9034-3b7a9b08b0bd",
   "metadata": {},
   "source": [
    "### 7. ëª¨ë¸ í›ˆë ¨í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08694a62-6674-4913-98d3-d4f7372d9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "patience = 2\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ & ì˜µí‹°ë§ˆì´ì €\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # íŒ¨ë”© í† í° ë¬´ì‹œ\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# PyTorch DataLoader ì„¤ì •\n",
    "train_dataset = TensorDataset(encoder_input_train, decoder_input_train, decoder_target_train)\n",
    "test_dataset = TensorDataset(encoder_input_test, decoder_input_test, decoder_target_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8c0fe-b7a7-4df8-82f8-85ad4ca428da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "patience = 2\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ & ì˜µí‹°ë§ˆì´ì €\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # íŒ¨ë”© í† í° ë¬´ì‹œ\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# PyTorch DataLoader ì„¤ì •\n",
    "train_dataset = TensorDataset(encoder_input_train, decoder_input_train, decoder_target_train)\n",
    "test_dataset = TensorDataset(encoder_input_test, decoder_input_test, decoder_target_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26e279a-5256-435e-b29a-357286213b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# í•™ìŠµ í•¨ìˆ˜\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs, patience):\n",
    "    model.train()\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for encoder_input, decoder_input, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # ì •ìˆ˜í˜• ë³€í™˜\n",
    "            encoder_input = encoder_input.to(device).long()\n",
    "            decoder_input = decoder_input.to(device).long()\n",
    "            target = target.to(device).long()\n",
    "\n",
    "            # ëª¨ë¸ ì‹¤í–‰\n",
    "            output = model(encoder_input, decoder_input)\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            target = target.view(-1)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # Validation loss ê³„ì‚°\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for encoder_input, decoder_input, target in test_loader:\n",
    "                encoder_input = encoder_input.to(device).long()\n",
    "                decoder_input = decoder_input.to(device).long()\n",
    "                target = target.to(device).long()\n",
    "\n",
    "                output = model(encoder_input, decoder_input)\n",
    "                output = output.view(-1, output.shape[-1])\n",
    "                target = target.view(-1)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping ì¡°ê±´\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8450db9-bad6-4039-bc50-32f149e5d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 30ë¶„ ì´ìƒ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. GPU í™˜ê²½ì—ì„œ í•™ìŠµí•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n",
    "# í•™ìŠµ ì‹¤í–‰\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, epochs=epochs, patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2c410a-4a8b-4d7d-a3fc-c9ad155f6731",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_losses)), train_losses, label='Train Loss')\n",
    "plt.plot(range(len(val_losses)), val_losses, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24896527-69aa-4bc2-a225-e48a4573e86b",
   "metadata": {},
   "source": [
    "### 8. ì¸í¼ëŸ°ìŠ¤ ëª¨ë¸ êµ¬í˜„í•˜ê¸°\n",
    "- ì¸í¼ëŸ°ìŠ¤ ëª¨ë¸ : ì´ë¯¸ í•™ìŠµì´ ëë‚œ ëª¨ë¸ì„ ì´ìš©í•´ ìƒˆë¡œìš´ ì…ë ¥ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ì„ ë§í•©ë‹ˆë‹¤.\n",
    "- ì‹œí€€ìŠ¤ íˆ¬ ì‹œí€€ìŠ¤ëŠ” í›ˆë ¨í•  ë•Œì™€ ì‹¤ì œ ë™ì‘í•  ë•Œì˜ ë°©ì‹ì´ ë‹¬ë¼ì„œ ê·¸ì— ë§ê²Œ ëª¨ë¸ ì„¤ê³„ë¥¼ ë”°ë¡œ í•´ì¤˜ì•¼ í•œë‹¤ê³  í•©ë‹ˆë‹¤.\n",
    "- í›ˆë ¨ ë•ŒëŠ” ì¸ì½”ë”ê°€ ì…ë ¥ ë¬¸ì¥ì„ ì¸ì½”ë”©í•˜ê³  ë””ì½”ë”ê°€ ì •ë‹µ ì‹œí€€ìŠ¤ì˜ ì´ì „ ë‹¨ì–´ë¥¼ ë°›ì•„ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n",
    "- í•˜ì§€ë§Œ ì‹¤ì œ ìƒí™©ì—ì„œëŠ” ì •ë‹µì´ ì—†ì–´ì„œ ë””ì½”ë”ê°€ ì§ì „ì— ìì‹ ì´ ì˜ˆì¸¡í•œ ë‹¨ì–´ë¥¼ ë‹¤ì‹œ ì…ë ¥ìœ¼ë¡œ ë„£ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e871868d-4136-4a5d-8fa4-0cb091cab8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = {idx: word for word, idx in src_vocab.items()} # ì›ë¬¸ ë‹¨ì–´ ì§‘í•©ì—ì„œ ì •ìˆ˜ -> ë‹¨ì–´ë¥¼ ì–»ìŒ\n",
    "tar_word_to_index = tar_vocab # ìš”ì•½ ë‹¨ì–´ ì§‘í•©ì—ì„œ ë‹¨ì–´ -> ì •ìˆ˜ë¥¼ ì–»ìŒ\n",
    "tar_index_to_word = {idx: word for word, idx in tar_vocab.items()} # ìš”ì•½ ë‹¨ì–´ ì§‘í•©ì—ì„œ ì •ìˆ˜ -> ë‹¨ì–´ë¥¼ ì–»ìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb44d5d-1b3f-4922-a0d1-0211f81c169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "decoder.to(device)\n",
    "\n",
    "# ì¸ì½”ë” ì„¤ê³„\n",
    "def encode_input(encoder, input_seq):\n",
    "    encoder_outputs, hidden, cell = encoder(input_seq)\n",
    "    return encoder_outputs, hidden, cell\n",
    "\n",
    "# ì´ì „ ì‹œì ì˜ ìƒíƒœë“¤ì„ ì €ì¥í•˜ëŠ” í…ì„œ\n",
    "num_layers = 3  # ë””ì½”ë” LSTM ë ˆì´ì–´ ê°œìˆ˜ (ì„¤ì •ì— ë§ì¶° ì¡°ì •)\n",
    "batch_size = 1\n",
    "\n",
    "decoder_state_input_h = torch.zeros((num_layers, batch_size, hidden_size), dtype=torch.float, device=device)\n",
    "decoder_state_input_c = torch.zeros((num_layers, batch_size, hidden_size), dtype=torch.float, device=device)\n",
    "decoder_input = torch.zeros((batch_size, 1), dtype=torch.long, device=device)\n",
    "\n",
    "dec_emb2 = decoder.embedding(decoder_input)\n",
    "\n",
    "# ë¬¸ì¥ì˜ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ì„œ ì´ˆê¸° ìƒíƒœ(initial_state)ë¥¼ ì´ì „ ì‹œì ì˜ ìƒíƒœë¡œ ì‚¬ìš©. ì´ëŠ” ë’¤ì˜ í•¨ìˆ˜ decode_sequence()ì— êµ¬í˜„\n",
    "# í›ˆë ¨ ê³¼ì •ì—ì„œì™€ ë‹¬ë¦¬ LSTMì˜ ë¦¬í„´í•˜ëŠ” ì€ë‹‰ ìƒíƒœì™€ ì…€ ìƒíƒœì¸ state_hì™€ state_cë¥¼ ë²„ë¦¬ì§€ ì•ŠìŒ.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder(decoder_input, decoder_state_input_h, decoder_state_input_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a2cc5d-53e0-45f5-891d-67d04e4c4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, decoder, attention, hidden_size, vocab_size):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.decoder = decoder  # ê¸°ì¡´ ë””ì½”ë”\n",
    "        self.attention = attention  # ì–´í…ì…˜ ë ˆì´ì–´\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)  # ì–´í…ì…˜ ê²°í•© ë ˆì´ì–´\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size)  # ìµœì¢… ì¶œë ¥ì¸µ\n",
    "        self.softmax = nn.Softmax(dim=-1)  # ì†Œí”„íŠ¸ë§¥ìŠ¤\n",
    "\n",
    "    def forward(self, decoder_inputs, decoder_hidden_state, decoder_state_h, decoder_state_c):\n",
    "        # ë””ì½”ë” ì‹¤í–‰\n",
    "        decoder_outputs, state_h, state_c = self.decoder(decoder_inputs, decoder_state_h, decoder_state_c)\n",
    "\n",
    "        # ì–´í…ì…˜ ì ìš©\n",
    "        attn_out = self.attention(decoder_outputs, decoder_hidden_state)\n",
    "\n",
    "        # ì–´í…ì…˜ê³¼ ë””ì½”ë” ì¶œë ¥ ê²°í•©\n",
    "        decoder_concat_output = torch.cat((decoder_outputs, attn_out), dim=-1)\n",
    "        decoder_concat_output = torch.tanh(self.concat(decoder_concat_output))\n",
    "\n",
    "        # ìµœì¢… ì¶œë ¥ì¸µ ì ìš©\n",
    "        decoder_outputs2 = self.softmax(self.output_layer(decoder_concat_output))\n",
    "\n",
    "        return decoder_outputs2, state_h, state_c\n",
    "\n",
    "# ê¸°ì¡´ Attention í´ë˜ìŠ¤ ì‚¬ìš©\n",
    "attention_layer = Attention_dot(hidden_size)\n",
    "\n",
    "# ë””ì½”ë” ëª¨ë¸ ìƒì„±\n",
    "decoder_model = DecoderWithAttention(decoder, attention_layer, hidden_size, tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82388b-baaf-4c7a-ae78-1df7cac707b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder, decoder, tar_word_to_index, tar_index_to_word, text_max_len, summary_max_len, device):\n",
    "    # ì…ë ¥ì„ PyTorch Tensorë¡œ ë³€í™˜\n",
    "    input_seq = torch.tensor(input_seq, dtype=torch.long, device=device)\n",
    "\n",
    "    # ì¸ì½”ë” ì‹¤í–‰í•˜ì—¬ ì´ˆê¸° ìƒíƒœ(hidden, cell) ì–»ê¸°\n",
    "    with torch.no_grad():\n",
    "        e_out, e_h, e_c = encoder(input_seq)\n",
    "\n",
    "    e_out = e_out.repeat(1, text_max_len, 1)  # ì°¨ì› ì¡°ì • (np.tile ëŒ€ì‹  repeat ì‚¬ìš©)\n",
    "\n",
    "    # <SOS>ì— í•´ë‹¹í•˜ëŠ” í† í° ìƒì„±\n",
    "    target_seq = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        # ë””ì½”ë” ì‹¤í–‰\n",
    "        with torch.no_grad():\n",
    "            output_tokens, h, c = decoder(target_seq, e_h, e_c)\n",
    "\n",
    "        # ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ë‹¨ì–´ ì„ íƒ\n",
    "        sampled_token_index = torch.argmax(output_tokens[0, -1, :]).item()\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if sampled_token != 'eostoken':\n",
    "            decoded_sentence += ' ' + sampled_token\n",
    "\n",
    "        # ì¢…ë£Œ ì¡°ê±´: <eos>ì— ë„ë‹¬í•˜ê±°ë‚˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ë©´ ì¤‘ë‹¨\n",
    "        if sampled_token == 'eostoken' or len(decoded_sentence.split()) >= (summary_max_len - 1):\n",
    "            stop_condition = True\n",
    "\n",
    "        # ê¸¸ì´ê°€ 1ì¸ íƒ€ê²Ÿ ì‹œí€€ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸\n",
    "        target_seq = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # ìƒíƒœ ì—…ë°ì´íŠ¸\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e13ff04-26c2-4014-8750-af4581b72809",
   "metadata": {},
   "source": [
    "### 9. ëª¨ë¸ í…ŒìŠ¤íŠ¸í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd5adc-c6e2-4911-b352-dc4ff9d10fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì›ë¬¸ì˜ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "def seq2text(input_seq):\n",
    "    temp = ''\n",
    "    for i in input_seq:\n",
    "        key = int(i.item())  # PyTorch Tensor â†’ int ë³€í™˜\n",
    "        if key != 0:  # íŒ¨ë”©(0) ì œì™¸\n",
    "            temp = temp + src_index_to_word.get(key, \"<UNK>\") + ' '  # ì•ˆì „í•œ ì¡°íšŒ\n",
    "    return temp.strip()\n",
    "\n",
    "# ìš”ì•½ë¬¸ì˜ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "def seq2summary(input_seq):\n",
    "    temp = ''\n",
    "    for i in input_seq:\n",
    "        key = int(i.item())  # PyTorch Tensor â†’ int ë³€í™˜\n",
    "        if key != 0 and key != tar_word_to_index['sostoken'] and key != tar_word_to_index['eostoken']:\n",
    "            temp = temp + tar_index_to_word.get(key, \"<UNK>\") + ' '  # ì•ˆì „í•œ ì¡°íšŒ\n",
    "    return temp.strip()  # ì–‘ìª½ ê³µë°± ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efce7dd-4b20-47c0-935d-98187e11a5ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(50, 100):\n",
    "    print(\"ì›ë¬¸ :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"ì‹¤ì œ ìš”ì•½ :\", seq2summary(decoder_input_test[i]))\n",
    "    input_seq = torch.tensor(encoder_input_test[i], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    print(\"ì˜ˆì¸¡ ìš”ì•½ :\", decode_sequence(input_seq, encoder, decoder, tar_word_to_index, tar_index_to_word, text_max_len, summary_max_len, device))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f160be-bf5f-46aa-9059-f4b95fe0d481",
   "metadata": {},
   "source": [
    "### 10. ì¶”ì¶œì  ìš”ì•½ í•´ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39797ecb-a1a9-4561-9564-dda3369794f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80176fc-8b7f-4b90-b455-2d8a1d2dd868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "import requests\n",
    "from summa.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c0f8c-0aed-488a-b901-8bfb4bff490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¼ë¶€ë§Œ ì¶œë ¥í•´ë³´ê¸°\n",
    "\n",
    "print(data[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c62f79-397c-468e-922d-25ad6506a677",
   "metadata": {},
   "source": [
    "#### Summa - summarize()\n",
    "\n",
    "- ```text (str)``` : ìš”ì•½í•  í…ŒìŠ¤íŠ¸.\n",
    "- ```ratio (float, optional)``` : ìš”ì•½ë¬¸ì—ì„œ ì›ë³¸ì—ì„œ ì„ íƒë˜ëŠ” ë¬¸ì¥ ë¹„ìœ¨. (0~1 ì‚¬ì´ ê°’)\n",
    "- ```words (int or None, optional)``` : ì¶œë ¥ì— í¬í•¨í•  ë‹¨ì–´ ìˆ˜. ë§Œì•½, ratioì™€ í•¨ê»˜ ë‘ íŒŒë¼ë¯¸í„°ê°€ ëª¨ë‘ ì œê³µë˜ëŠ” ê²½ìš° ratioëŠ” ë¬´ì‹œí•©ë‹ˆë‹¤.\n",
    "- ```split (bool, optional)``` : Trueë©´ ë¬¸ì¥ list / FalseëŠ” ì¡°ì¸(join)ëœ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "- Summaì˜ summarizeëŠ” ë¬¸ì¥ í† í°í™”ë¥¼ ë³„ë„ë¡œ í•˜ì§€ ì•Šë”ë¼ë„ ë‚´ë¶€ì ìœ¼ë¡œ ë¬¸ì¥ í† í°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. (ì›ë¬¸ì„ ì…ë ¥ìœ¼ë¡œ ë„£ì„ ìˆ˜ ìˆìŒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bacee9-1d81-412c-ab50-2b9d56495e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd46393-c251-4169-81d2-8bc3dd878713",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed1b58-8bb2-4054-9ff4-6faa6ed74038",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.01, split=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb7fbd-0194-4af8-a365-a5461b43baf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ì–´ ìˆ˜ë¥¼ ì„¤ì •í•˜ì—¬ ìš”ì•½ë¬¸ì˜ í¬ê¸° ì¡°ì ˆ ê°€ëŠ¥\n",
    "\n",
    "print('Summary:')\n",
    "print(summarize(text, words=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4922ef-94ec-4969-9d70-4fcda8b799af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
